# 9/2~9/9 TIL âœï¸

## íƒœìš± [[Github](https://github.com/K-ple)]

-

## ìƒìœ  [[Github](https://github.com/dhfpswlqkd)]

-

## ì§€í˜„ [[Github](https://github.com/jihyun-0611)]

- Translate one modality to a concept
  - ì‚¬ëŒì€ ì—¬ëŸ¬ í˜•íƒœì˜ ì •ë³´ë¥¼ ë°›ì•˜ì„ ë•Œ ë‡Œì˜ ë‰´ëŸ°ì´ í™œì„±í™”ë¨
  - ì´ë•Œ, ì–´ë–¤ í•˜ë‚˜ì˜ ì •ë³´ë¥¼ ë‹¤ì–‘í•œ modalityë¡œ ë°›ì•˜ì„ ë•Œ ëª¨ë‘ ê°™ì€ ë‰´ëŸ°ì´ í™œì„±í™”ë¨ (ì˜¤í”„ë¼ìœˆí”„ë¦¬ ë‰´ëŸ°)
  - ë”°ë¼ì„œ ì •ë³´ê°€ ì–´ë–¤ modalityë¡œ ë“¤ì–´ì˜¤ë“  í•˜ë‚˜ì˜ ê°œë…ìœ¼ë¡œì„œ ì €ì¥ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
- Multisensory fusion for concept learning
  - ë©€í‹° ëª¨ë‹¬ì€ ìœ„ì˜ ì‚¬ëŒì˜ ì¸ì§€ ë°©ì‹ì— ê¸°ë°˜í•´ multisensory fusionì„ í†µí•´ conceptì„ í•™ìŠµ ì‹œí‚¨ë‹¤.
- Multimodal Categories
  ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/fa504c93-6e3e-4bc6-9ba7-bdbaf1ad9e33/ab9eb4b8-aabd-4f23-9d43-28cb91c19189/image.png)
- Data representation
  - Image data structure
    - A 2D image is represented by an intensity value of each pixel in 2D array structure
    - color image patch: 3D pathch - HxWxC
  - Video data structure
    - A stack of image frames
  - 3D data structure
    - Various data representation : Multi-view images, Volumetric(voxel), Part assembly, Point cloud, Mesh, Implicit shape
  - Text embedding
    - Characters are hard to use in machine learning
    - Map words (or tokens) to dense vectors
    **Token**
    - Numerical representation of words
    - Pre-fixed vector embedding (like word dictionary)
    **Tokenization**
    - Split the text data into small units â€“ tokens.
    - â€˜Numbersâ€™ can be processed easily than â€˜textâ€™
    - Token ID to embedding through embedding layer
    - Token, not word
    - Embedding layer(look-up-table)
      - Input: index(sparse)
      - Output: token embed.
    - Embeddings are learned from scratch
    **Word2Vec â€“skip-gram model**
    - Trained to learn W and Wâ€™
    - Rows in W represent word embedding vectors
    - Learning to predict neighboring ğ‘ words for understanding relationships between words
      â†’ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì¸ì ‘í•œ ğ‘ words ì˜ˆì¸¡ì„ í•™ìŠµ
    - Given a model with a window of size 5, the center words depend on 4 words
      â†’ window í¬ê¸°ê°€ 5ì¸ ëª¨ë¸ì—ì„œ ì¤‘ì‹¬ ë‹¨ì–´ëŠ” 4ê°œì˜ ë‹¨ì–´ì— ë”°ë¼ ë‹¬ë¼ì§
    - Emerging semantic relationship
  - Sound representation
    - Acoustic feature extraction from waveform to spectrogram
    - Short-time Fourier transform (STFT)
      : Fourier transform (FT) on windowed waveform results in frequency-magnitude graph
      â†’ waveformì— windowë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ë³„ë¡œ ìë¥¸ë’¤ í•´ë‹¹ ë¶€ë¶„ì— í‘¸ë¦¬ì— ë³€í™˜ì„ ì‚¬ìš©í•˜ë©´ frequency-magnitude graphë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
      - FT decomposes an input signal into constituent frequencies : í‘¸ë¦¬ì—ë³€í™˜ì€ ì…ë ¥ ì‹ í˜¸ë¥¼ êµ¬ì„± ì£¼íŒŒìˆ˜ë¡œ ë¶„í•´í•¨.
    - Spectrogram: A stack of spectrums along the time axis (ìœ„ì˜ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë¶„í•´í•œ ê·¸ë˜í”„ë¥¼ ìŠ¤íƒí•˜ë©´ ìŠ¤í™í† ê·¸ë¨ì´ ë¨)
- Multi-modal alignment(matching )
  Applicationâ€“Image tagging
  - Can find relevant tags of a given image, or retrieve images by a query keyword
  **CLIP**: Contrastive Language-Image Pre-training, by OpenAI
  - Learn visual concepts from the natural language supervision
  - Train with a wide variety of images and natural language pairs
    - 400 million (image, text) pairs collected from internet
  - Architecture
    â€“ Image encoder : ViT-B (or ResNet50) - ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  í•´ë‹¹ ì´ë¯¸ì§€ì˜ ê³ ì • í¬ê¸° ë²¡í„° í‘œí˜„ì„ ìƒì„±
        â€“ Text encoder : Transformers

        - í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ì˜ ê³ ì • í¬ê¸° ë²¡í„° í‘œí˜„ì„ ì¶œë ¥

        â†’ í…ìŠ¤íŠ¸ ì¸ì½”ë”ì™€ ì´ë¯¸ì§€ ì¸ì½”ë” ëª¨ë‘ ê°ê°ì˜ ì…ë ¥ì„ ë™ì¼í•œ ì°¨ì›ì˜ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‘ ì–‘ì‹ì„ ë¹„êµ

        â†’ CLIP exhibits domain-robust performance
  **Contrastive learning**
  - Pull a target image (anchor) to a matching image (positive)
  - Push an anchor from many non-matching images (negative)
  - Given a batch of ğ‘(image, text) pairs
    - Predict embeddings for each modality : ê° modalityì— ëŒ€í•œ embedding ì˜ˆì¸¡
    - Compute ğ‘Ã—ğ‘cosine similarities : ë‘ ê°œì˜ embeddingì— ëŒ€í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•¨
      cosine similarity:
      $$
      \langle I_i, T_i \rangle = \frac{I_iT_i}{\|I_i\|\|T_i\|}
      $$
  **Pre-training method for CLIP**
  - contrasitive learning objective:
    - Maximize the cosine similarities of the N correct embedding pairs
      â†’ Nê°œì˜ correct embedding pairsì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ contrastive learning ëª©í‘œ
    - Minimize the cosine similarities of the ($N^2 - N$) incorrect pair
      â†’ $N^2 - N$ ê°œì˜ incorrect pairì— ëŒ€í•´ì„œëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìµœì†Œí™”
    â‡’ Optimize a symmetric cross-entropy loss over the similarity scores
    - ëŒ€ì¡° ì†ì‹¤ì„ ì‚¬ìš©í•˜ì—¬ ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì´ ì˜ëª»ëœ ìŒë³´ë‹¤ ë” ë†’ì€ ìœ ì‚¬ì„± ì ìˆ˜ë¥¼ ê°–ë„ë¡ ìœ ë„í•¨.
    - ì†ì‹¤ì€ ì–‘ë°©í–¥ì— ëŒ€í•´ ê³„ì‚°
      - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸: ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ëª¨ë¸ì€ ë°°ì¹˜ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì„¤ëª… ì¤‘ì—ì„œ ì˜¬ë°”ë¥¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ëŠ”ë‹¤.
      - í…ìŠ¤íŠ¸-ì´ë¯¸ì§€: ê° í…ìŠ¤íŠ¸ ì„¤ëª…ì— ëŒ€í•´ ëª¨ë¸ì€ ë°°ì¹˜ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì¤‘ì—ì„œ ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ”ë‹¤.
  **Diverse applications using pre-trained CLIP**
  - Image captioning
  - Image stylization with language
  - Image/video retrieval with text
  - Text-to-image generation
  - CLIP-guided motion generation
  - CLIP-guided 3D object/mesh generation
  - â€¦
  - (18,000 citations within 3 years)

## ìœ¤ì„œ [[Github](https://github.com/myooooon)]

-

## ì„¸ì—° [[Github](https://github.com/Yeon-ksy)] [[Velog](https://velog.io/@yeon-ksy/)]

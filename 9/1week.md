# 9/2~9/9 TIL âœï¸

## íƒœìš± [[Github](https://github.com/K-ple)
### Multimodal
- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„±, ì˜ìƒ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ì–‘ì‹ (modality)ì„ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ ì˜ë¯¸

### CLIP
- ViT(Vision Transformer)ì™€ Transformer ì–¸ì–´ ëª¨ë¸(Transformer-based language model)ì„ ê²°í•©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ë†“ì€ ëª¨ë¸
- ì¦‰, Textì™€ Imageì˜ ê´€ê³„ì„±ì„ ëª¨ë¸ë§

![taeuk_image1.png](image%2Ftaeuk_image1.png)
- 4ì–µê°œ ì´ë¯¸ì§€ì™€ í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª… Textë¥¼ pairë¡œ ë‘” í•™ìŠµë°ì´í„°ì…‹ì„ ê° ì¸ì½”ë”(Text,Image)ë¡œ ì„ë² ë”© í›„ ìœ„ì‚¬ì§„ê³¼ ê°™ì´ pairì˜ ê±°ë¦¬ì—ë”°ë¼ ìœ ì‚¬ë„ë¥¼ ê³„ì‚° í•¨


![taeuk_image2.png](image%2Ftaeuk_image2.png)
- Textì˜ ê²½ìš° A photo of a {object}ì™€ ê°™ì´ ë‹¨ì–´ í˜•íƒœê°€ ì•„ë‹Œ ë¬¸ì¥ í˜•íƒœë¡œ ì¸ì½”ë”©ì‹œ ì„±ëŠ¥ í–¥ìƒì˜ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŒ
- 


## ìƒìœ  [[Github](https://github.com/dhfpswlqkd)]
- 

## ì§€í˜„ [[Github](https://github.com/jihyun-0611)]
-

## ìœ¤ì„œ [[Github](https://github.com/myooooon)]
 ## [CV ì´ë¡ ] 
 4. Segmentation & Detection


## Semantic segmentation
ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒ (instanceëŠ” ê³ ë ¤í•˜ì§€ ì•Šê³  ì˜ë¯¸ì  ì¹´í…Œê³ ë¦¬ë§Œ ê³ ë ¤)
- ì ìš© ë¶„ì•¼ : Medical images, Autonomous driving, Computational photography

### 1. Fully Convolutional Networks(FCN)

<img src="https://github.com/user-attachments/assets/5413e4de-4488-4ab6-a350-a4364a493ab0" width=550/>

- semantic segmentationì˜ ì²« end-to-end êµ¬ì¡° ëª¨ë¸
- FCNì€ ì „ì²´ ì‚¬ì§„ì˜ classë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ íŠœë‹í•´ í”½ì…€ ë‹¨ìœ„ì˜ classë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” Transfer Learningìœ¼ë¡œ êµ¬í˜„í•œë‹¤.
    - ì´ë¯¸ì§€ ë¶„ë¥˜ì˜ Fully Connected layerëŠ” ê³ ì •ëœ ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì¶œë ¥í•˜ë©° ê³µê°„ì  ì •ë³´ë¥¼ ë‹´ì§€ ëª»í•œë‹¤. FC ë ˆì´ì–´ ëŒ€ì‹  **1x1 conv**ì™€ **up-sampling**ì„ ì´ìš©í•´ ê³µê°„ì  ì •ë³´ë¥¼ ê°€ì§€ë©° inputê³¼ í¬ê¸°ê°€ ê°™ì€ classification mapì„ ì¶œë ¥í•œë‹¤.
        
- Up-sampling & Transposed convolution
    
    Conv layerë¥¼ í†µê³¼í•˜ë©° poolingì´ë‚˜ strideë¡œ ì¸í•´ í•´ìƒë„ê°€ ë‚®ì•„ì§„ feature mapì„ input sizeì™€ ê°™ì•„ì§€ë„ë¡ up-samplingí•œë‹¤.
    
    í•˜ì§€ë§Œ ìœ„ì¹˜ ì •ë³´ê°€ ì†ì‹¤ëœ feature mapì„ ê·¸ëŒ€ë¡œ up-samplingí•˜ë©´ ë””í…Œì¼í•œ classification mapì„ ì–»ì„ ìˆ˜ ì—†ë‹¤. ë” ë””í…Œì¼í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆëŠ” ì¤‘ê°„ level feature mapì„ ìµœì¢… feature mapì™€ ë”í•˜ëŠ” skip connection ì„ í†µí•´ ì´ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆë‹¤.   

    ![FCN_result](https://github.com/user-attachments/assets/3882c62e-1006-43ec-9e58-6722ff273f48)
    ![FCN_skipconnection](https://github.com/user-attachments/assets/2e7661ec-bb9a-4a7f-a7c0-12334f65b2c8)
    
    - FCN-32s
        
        ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ (H, W)ë¼ í•˜ë©´, (H/32, W/32) í¬ê¸°ì˜ pool5ë¥¼ 32ë°° up-sampling
        
    - FCN-16s
        
        pool5ë¥¼ 2ë°° up-samplingí•˜ì—¬ (H/16, W/16) í¬ê¸°ì˜ pool4ì™€ ë”í•œ ë‹¤ìŒ(=A), 16ë°° upsampling
        
    - FCN-8s
        
        FCN-16sì˜ map Aë¥¼ 2ë°° up-samplingí•˜ì—¬ (H/8, W/8) í¬ê¸°ì˜ pool3ê³¼ ë”í•œ ë‹¤ìŒ, 8ë°° up-sampling
        

### 2. U-Net

<img src="https://github.com/user-attachments/assets/8de12164-e03c-4f88-ba36-d4f652461906" width=550>

- U-Netì€ down-samplingí•˜ëŠ” contracting pathì™€ up-samplingí•˜ëŠ” expanding pathì˜ ëŒ€ì¹­ êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. Contracting pathì˜ feature mapì„ expanding pathì˜ feature mapì— ë”í•´ì£¼ëŠ” skip connectionì„ í†µí•´ localized ì •ë³´ë¥¼ ì „ë‹¬í•œë‹¤.
- Contracting path
    - 3 x 3 convì„ ë‘ ë²ˆ ì ìš© â†’ 2 x 2 max poolingìœ¼ë¡œ down-sampling
    - down-samplingí•  ë•Œ ì±„ë„ì˜ ìˆ˜ê°€ 2ë°°ë¡œ ëŠ˜ì–´ë‚œë‹¤.
- Expanding path
    - 2 x 2 up-convìœ¼ë¡œ up-sampling â†’ ëŒ€ì‘í•˜ëŠ” contracting pathì˜ feature map ë”í•˜ê¸° â†’ 3 x 3 convì„ ë‘ ë²ˆ ì ìš©
    - up-samplingí•  ë•Œ ì±„ë„ì˜ ìˆ˜ê°€ 1/2ë¡œ ì¤„ì–´ë“ ë‹¤.

## Object detection

Classification + Box localization

- ì ìš© ë¶„ì•¼ : Autonomous driving, Optical Character Recognition(OCR)

### 1. Two-stage detector : R-CNN

![R_CNN](https://github.com/user-attachments/assets/8785085b-b288-4d65-bbee-436d3484b17a)

- ê°ì²´ê°€ ìˆì„ë§Œí•œ ì˜ì—­ì„ ì œì•ˆí•´ì£¼ëŠ” region proposalê³¼ ê°ì²´ë¥¼ ë¶„ë¥˜í•˜ëŠ” classification ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” 2-stage object detection ëª¨ë¸
- R-CNNì€ ì‚¬ëŒì´ ë§Œë“  region proposal ì•Œê³ ë¦¬ì¦˜(Selective search)ì„ ì‚¬ìš©í•˜ì—¬ bounding boxë¥¼ ì¶”ì¶œí•œë‹¤. ì´ë¥¼ ê³ ì •ëœ ì‚¬ì´ì¦ˆë¡œ ë³€í˜•ì‹œí‚¤ê³ (warping) ë¯¸ë¦¬ í•™ìŠµëœ CNNì— ë„£ì–´ featureë¥¼ ì¶”ì¶œí•œë‹¤. ì´ featureë¥¼ SVM object classifierì— ë„£ì–´ ê°ì²´ë¥¼ ë¶„ë¥˜í•˜ê³ , Box offset regressorì— ë„£ì–´ bounding box offsetì„ ì¡°ì •í•œë‹¤.

### 2. One-stage detector : YOLO

<img src="https://github.com/user-attachments/assets/47f6c3a8-6f88-4e37-9966-9a35b6f471b2" width=550>

- ì´ë¯¸ì§€ë¥¼ í•œë²ˆë§Œ ë³´ê³  region proposalê³¼ classificationì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” 1-stage object detection ëª¨ë¸ë¡œ ì†ë„ê°€ ë¹¨ë¼ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì— ìœ ìš©í•˜ë‹¤.
- ì´ë¯¸ì§€ë¥¼ S x S gridë¡œ ë‚˜ëˆ„ê³  grid cellë§ˆë‹¤ Bê°œì˜ bounding box, ê° boxì— ëŒ€í•œ confidence(bounding box ì„  êµµê¸°), Cê°œì˜ conditional class probabilityë¥¼ ì˜ˆì¸¡í•œë‹¤.  ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ëŠ” S x S x (B x 5 + C)í¬ê¸°ì˜ tensorë¡œ ì¸ì½”ë”©ëœë‹¤(B x 5 + C â†’ bounding boxì˜ x, y, w, h, obj score + class probability). ì´ë•Œ ì˜ˆì¸¡ëœ bounding boxë“¤ ì¤‘ ê° ê°ì²´ì— ê°€ì¥ ì •í™•í•œ í•˜ë‚˜ì˜ ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´ Non-Maximum Suppression(NMS) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œë‹¤.

### 3. One-stage detector vs. Two-stage detector

- One-stage (YOLO, RetinaNetâ€¦)
    - No explicit RoI pooling
    - Class imbalance problem - One-stageëŠ” ëª¨ë“  pixelì— lossë¥¼ ê³„ì‚°í•˜ëŠ”ë°, ê°ì²´ê°€ ìˆëŠ” positive anchor boxë³´ë‹¤ ë°°ê²½ ì˜ì—­ì— í•´ë‹¹í•˜ëŠ” negative anchor boxê°€ ë” ë§ì•„ loss ê³„ì‚°ì— ì–´ë ¤ì›€ì´ ìˆë‹¤.
        
        â†’ focal lossë¡œ ê°œì„  (focal lossëŠ” ground truth classì¼ í™•ë¥ ì´ ë†’ìœ¼ë©´ down-weights, ë‚®ìœ¼ë©´ over-weightsí•˜ëŠ” ë°©ë²•)
        
- Two-stage(R-CNN, Fast R-CNN, Faster R-CNNâ€¦)
    - Regioin proposalë¡œ ì œì•ˆëœ box ì˜ì—­ì„ ê°€ì ¸ì™€ì„œ ê³ ì •ëœ í¬ê¸°ì— ë§ê²Œ ì¡°ì ˆí•˜ëŠ” RoI poolingì´ ì¡´ì¬í•œë‹¤.


## Instance segmentation

semantic segmentation + distinguishing instances (ë°°ê²½ì€ labelì„ ë¶€ì—¬í•˜ì§€ ì•ŠìŒ)

### 1. Mask R-CNN
- Faster R-CNNì€ RoI poolingì„ í†µí•´ í¬ê¸°ê°€ ì¤„ì–´ë“  featureê°€ RoIë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” misalignment ë¬¸ì œê°€ ìˆë‹¤. Mask R-CNNì€ RoI pooling ëŒ€ì‹  RoI alignì„ ì‚¬ìš©í•´ floating pointê¹Œì§€ ê³ ë ¤í•œ ë” ì •êµí•œ ëª¨ë¸ì´ë‹¤. ë˜í•œ, ë§ˆì§€ë§‰ ì˜ˆì¸¡ ë‹¨ê³„ì—ì„œ Mask headë¥¼ ì¶”ê°€í•´ segmentation maskë¥¼ ì˜ˆì¸¡í•œë‹¤.
    - Extensions : DensePose R-CNN, Mesh R-CNN

## Transformer-based methods

### 1. DETR (Detection Transformer)

- End-to-End Object Detection with Transformers(encoder-decoder êµ¬ì¡°ë¥¼ ì‚¬ìš©)
- Object detectionì„ direct set prediction problemìœ¼ë¡œ ë§Œë“¤ë©´ì„œ ë§ì€ hand-designed componentë“¤ì„ ì—†ì•¨ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ê¸°ì¡´ object detection ëª¨ë¸ì—ì„œ RPNì´ ê°™ì€ ê°ì²´ì— ì—¬ëŸ¬ ê°€ì§€ bounding boxë¥¼ ë§Œë“¤ ë•Œ ìµœì ì˜ boxë¥¼ ì„ íƒí•´ì£¼ë˜ non-maximum suppressionë„ neural network ì•ˆìœ¼ë¡œ ë“¤ì–´ì˜¨ ê²ƒì´ë‹¤.
- Method
    1. Feature extraction with CNN + position encoding
    2. Transformer encoder
        - CNNì—ì„œ ì¶”ì¶œëœ feature mapì´ ë” ê°•í™”ëœ inceptive fieldë¥¼ ê³ ë ¤í•˜ë„ë¡ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.
    3. Transformer decoder
        - Encoder outputê³¼ object queriesë¥¼ inputìœ¼ë¡œ ê°€ì§„ë‹¤.
        - Object queriesëŠ” ì–´ë–¤ objectê°€ ì–´ë””ì— ìˆëŠ”ì§€ë¥¼ ë¬¼ì–´ë³¸ë‹¤.
        - ì¶œë ¥ì´ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” auto-regressive í˜•íƒœë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , Nê°œì˜ queryë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•´ Nê°œì˜ featureë¡œ ê°ê° decodingí•˜ë„ë¡ í•œë‹¤.
    4. Prediction heads(Feed forward network)
        - Nê°œì˜ featureê°€ FFNì„ ê±°ì³ classì™€ bounding boxì˜ í˜•íƒœë¡œ ì¶œë ¥
        - ì˜ˆì¸¡í•˜ëŠ” bounding boxì˜ ê°œìˆ˜ê°€ ì‹¤ì œ object ê°œìˆ˜ë³´ë‹¤ ë§ë„ë¡ queryë¥¼ ì„¤ì •í•œë‹¤.
        - class label â€˜Noneâ€™ì€ ê°ì²´ê°€ ì—†ìŒì„ ëœ»í•œë‹¤.
    5. Bipartite matching
        - Lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì˜ˆì¸¡í•œ boxì™€ ì‹¤ì œ boxë¥¼ matching
        - ì¶œë ¥ê²°ê³¼ê°€ í•œë²ˆì— ìˆœì„œì—†ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì–´ë–¤ labelê³¼ ëŒ€ì‘ë˜ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ bounding box prediction setê³¼ ground truth setì„ matchingí•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

### 2. MaskFormer

<img src="https://github.com/user-attachments/assets/028f7c40-28e8-4812-b6ac-b2aaffde4b72" >

- Mask classificationìœ¼ë¡œ semantic & instance segmentation ë‘ ê°€ì§€ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” insightì—ì„œ ì‹œì‘
- Pixel-level module
    - Backbone modelì´ low-resolution image featuresì„ ìƒì„±í•˜ê³  pixel decoderê°€ image featuresì„ up-samplingí•˜ì—¬ per-pixel embeddingì„ ì¶œë ¥í•œë‹¤.
- Transformer module
    - Transformer ëª¨ë¸ì˜ decoder ë¶€ë¶„ìœ¼ë¡œ image featuresì™€ positional embeddingsë¥¼ í•©ì¹œ ê°’ì„ inputìœ¼ë¡œ ë„£ì–´ Nê°œì˜ per-segment embeddingì„ ì¶œë ¥í•œë‹¤. DETRê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Nê°œì˜ queryë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
- Segmentation module
    - Per-segment embeddingì´ MLPë¥¼ ê±°ì³ ê° segmentì— ëŒ€í•œ classificationì„ ìˆ˜í–‰í•œë‹¤.
    - Per-segment embeddingì´ MLPë¥¼ ê±°ì³ mask embeddingìœ¼ë¡œ ë³€í™˜ëœë‹¤. Mask embeddingê³¼ per-pixel embeddingì„ dot productí•œ í›„ sigmoidí•¨ìˆ˜ë¥¼ ì ìš©í•´ binary mask predictionì„ ìˆ˜í–‰í•œë‹¤.
    - Classification lossì™€ binary mask lossê°€ DETRì²˜ëŸ¼ set predictionìœ¼ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì„œë¡œ matchingí•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

### 3. Uni-DVPS

<img src="https://github.com/user-attachments/assets/9c9d7fa4-f547-42c6-bc52-653b718cb3dc">

- ë¹„ë””ì˜¤ì—ì„œ panoptic segmentation(semantic segmentation + instance segmentation)ê³¼ depth predictionì„ í•œë²ˆì— ìˆ˜í–‰í•˜ëŠ” unified ëª¨ë¸
- MaskFormerì™€ ìœ ì‚¬í•˜ê²Œ feature extractorì™€ pixel decoder, transformer decoderë¥¼ ê°€ì§€ê³  ìˆë‹¤.
- Unified Transformer decoder with unified query
    - ì—¬ëŸ¬ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í†µí•©ëœ queryë¥¼ ì‚¬ìš©
    - Segmentationì„ ìœ„í•œ embeddingê³¼ depth predictionì„ ìœ„í•œ embeddingìœ¼ë¡œ ë¶„í™”ëœë‹¤.
- Feature gate
    - Pixel decoderì—ì„œ ë‚˜ì˜¨ feature mapë„ feature gateë¥¼ ê±°ì³ ë‘ taskì— ê°ê° ìœ ë¦¬í•œ featureë¡œ decodingëœë‹¤.
- Video segmentationì„ ìœ„í•´ì„œëŠ” ì‹œê°„ì— ë”°ë¥¸ ì¶”ì ì´ í•„ìš”í•˜ë‹¤. Uni-DVPSì—ì„œëŠ” tracking moduleì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  query-based trackingì„ ì‚¬ìš©í•œë‹¤.
    - ê°™ì€ instanceëŠ” ì—¬ëŸ¬ frameì— ê±¸ì³ ë¹„ìŠ·í•œ query featureê°€ ë‚˜íƒ€ë‚˜ê³  ë‹¤ë¥¸ instanceëŠ” ì„œë¡œ êµ¬ë¶„ë˜ëŠ” query featureê°€ ë‚˜íƒ€ë‚˜ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. Frame ê°„ query matchingì„ í†µí•´ video trackingì„ ìˆ˜í–‰í•œë‹¤.

## ì„¸ì—° [[Github](https://github.com/Yeon-ksy)] [[Velog](https://velog.io/@yeon-ksy/)]

# [CV ì´ë¡ ] Multimodal

## 1. CLIP

- í…ìŠ¤íŠ¸ í†µí•´ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ.
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í†µí•´ ê°ê°ì˜ íŠ¹ì§• ë²¡í„°ë¥¼ ë¹„êµ ëŒ€ì¡°
    
    <img src="https://github.com/user-attachments/assets/1cec8297-7784-418e-9745-e36de558cfbe" width="500"/>
    
    - ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ í…ìŠ¤íŠ¸ ì¸ì½”ë”ì˜ Joint embeddingì„ í•™ìŠµí•˜ì—¬ ê´€ê³„ì„±ì„ í•™ìŠµ
        - ì´ë¯¸ì§€ ì¸ì½”ë” : ViT-B (ë˜ëŠ” ResNet50)
        - í…ìŠ¤íŠ¸ ì¸ì½”ë” : íŠ¸ëœìŠ¤í¬ë¨¸
- loss (Contrastive learningì„ ì‚¬ìš©)
    - ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, cosine similarityì„ í†µí•´ ê´€ë ¨ ìˆëŠ” featureëŠ” ë‹¹ê¸°ê³ , ê´€ë ¨ ì—†ëŠ” ê²ƒì€ ë–¨ì–´íŠ¸ë¦¼.
    
    <img src="https://github.com/user-attachments/assets/8423e0b6-81af-47b1-ab10-028bc67901df" width="500"/>
    
    - pseudo code
        
        <img src="https://github.com/user-attachments/assets/25d43781-1a4b-4268-8ea4-ee3d13ddff7a" width="500"/>
        
        - logits ê³„ì‚°ì— `np.exp(t)`ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°, ì´ê²ƒì€ temperatureë¡œ cross-entropyì„ ì–´ëŠ ì •ë„ ë¯¼ê°ë„ë¡œ í•™ìŠµì„ ì§„í–‰í•  ì§€ ì •í•¨.

### 1.1. CLIP í™œìš© :  ZeroCap

- GPT2ì™€ CLIPì„ ê²°í•©í•˜ì—¬ ì¶”ê°€ì ì¸ trainingì—†ì´ ìº¡ì…˜í•¨.

    
- Method
    - ë‹¨ì–´ì—ì„œ ë‚˜ì˜¨ íŠ¹ì„±ë“¤ê³¼ ì´ë¯¸ì§€ì™€ ì˜ ë§ëŠ” ì§€ ì¸¡ì • ($l_{CILP}$)
        
        <img src="https://github.com/user-attachments/assets/f04a9a54-ab12-40c6-891d-40e941f63eae" width="500"/>
        
        - ì´ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ì—ì„œ visual featureì„ ë§Œë“¤ì–´ ë‹¨ì–´ íŠ¹ì„±ê³¼ ë¹„êµ
            - ë§Œì•½ ì°¨ì´ê°€ ìˆìœ¼ë©´, ì—­ì „íŒŒë¥¼ í†µí•´ì„œ Contextë¥¼ ì—…ë°ì´íŠ¸
    - ë‹¤ìŒ ë‹¨ì„ ì˜ˆì¸¡í•  ë•Œ, ê¸°ì¡´ì— ìˆë˜ ë²¡í„°ë“¤ì´ ë³€í•˜ì§€ ì•Šë„ë¡ ìœ ì§€í•˜ëŠ” cross-entropy lossë¥¼ ì¶”ê°€
        
        <img src="https://github.com/user-attachments/assets/b5520a17-d163-4bc5-8709-4bab158fda20" width="500"/>
        

### 1.2. CLIP í™œìš© : ImageBIND

- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, dept, IMU, audio ë“± ë‹¤ì–‘í•œ ì„¼ì„œì™€ì˜ ê´€ê³„ë¥¼ í†µí•©.

### 1.3. CLIP í™œìš© : DALL-E 2

- Text-to-Image generationì„.
- CLIP & diffusion modelsë¡œ ì´ë£¨ì–´ì§.
- ë¨¼ì € í•™ìŠµí•œ CLIPê³¼ diffusion ëª¨ë¸ì„ ë³„ë„ë¡œ ìƒì„±í•´ì„œ ì—°ê²°í•˜ëŠ” ëª¨ë“ˆ ë°©ì‹ì„.
    
    <img src="https://github.com/user-attachments/assets/f9265e76-c8c9-4d49-97fb-d50e3fad8936" width="500"/>
    

## 2. Visual-language model

### 2.1. Show, attend and tell

- ì´ë¯¸ì§€ì˜ ì¼ë¶€ ì˜ì—­ì„ ì°¸ì¡°í•´ì„œ ìº¡ì…˜ì„ ìƒì„±
    
    <img src="https://github.com/user-attachments/assets/cdf8361b-2fb9-44f7-9f75-ffe0a878bc99" width="500"/>
    
    - ì´ë¯¸ì§€ì— ConvNetì„ ì¨ì„œ featureì„ ìƒì„±í•˜ê³ , LSTMì„ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ê°€ ì‚¬ì§„ì˜ ì–´ë–¤ ë¶€ë¶„ì„ ì°¸ê³ ë¥¼ í•´ì„œ ë‚˜ì™€ì•¼í•˜ëŠ”ì§€ ì˜ˆì¸¡í•˜ì—¬ attentionê³¼ ìº¡ì…”ë‹ì„ ìˆ˜í–‰
- method
    
    <img src="https://github.com/user-attachments/assets/a4a1c3e6-0d56-4737-93f3-b5c81c0cbd46" width="500"/>
    
    - $s_1$ = ì–´ë–¤ ë¶€ë¶„ì„ ì°¸ì¡°í•´ì„œ ìº¡ì…˜ì„ ì‹œì‘í•˜ëŠ” ì§€ attetion ë§µì„ ì¶”ì¶œ.
    - ì´ attention ë§µê³¼ featureì„ ì„œë¡œ weighted combinationì„ í†µí•´ feature $z$ë¥¼ ì¶”ì¶œ.
    - ì´ hidden stateë¡œë¶€í„° ì–´ë–¤ ë‹¨ì–´ê°€ ì˜ˆì¸¡ë˜ì–´ì•¼ í•˜ëŠ” ë‹¨ì–´ë¥¼ ë””ì½”ë”©í•˜ê³  (d$_1$) ë‹¤ìŒ ë‹¨ì–´ê°€ ì°¸ê³ í•´ì•¼ í•˜ëŠ” ì˜ì—­ì„ attentionì„ í•¨.
    - $z_2$ì„ ê³„ì‚°í•˜ê³  $h_2$ë¥¼ ìƒì„±. (ì´ì „ ë‹¨ì–´ê°€ ê°™ì´ ë“¤ì–´ê°€ì„œ $h_2$ë¥¼ ìƒì„±)

### 2.2. Flamingo

- Transformerëª¨ë¸ì¸ Chinchillaí™œìš©
    
    <img src="https://github.com/user-attachments/assets/8ac9edd7-02bc-48bf-8379-9ce049c67f79" width="500"/>
    
    - pre-trainingì´ ëœ ë ˆì´ì–´ì€ fixí•˜ê³  (ëˆˆê½ƒ í‘œì‹œ), Learnableí•œ ë ˆì´ì–´ë¥¼ (ëˆˆê½ƒ ë°‘ ì—°ë³´ë¼ìƒ‰) ì‚½ì…í•˜ì—¬ ì´ ë¶€ë¶„ë§Œ í•™ìŠµì„ í•¨.
    - Vision Encoderì„ í†µí•´ featureì„ ë½‘ê³ , Language Modelì˜ ë ˆì´ì–´ì— ì—°ê²°í•¨.
        - ì´ ë•Œ, perceiver resamperì„ ì‚¬ìš©í•˜ëŠ” ë°, ì´ëŠ” input ì´ë¯¸ì§€ì˜ ì‚¬ì´ì¦ˆê°€ ë‹¤ì–‘í•œ ì‚¬ì´ì¦ˆë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ, í•­ìƒ fixed-sizedì„ ë°˜í™˜í•´ì¤Œ
        - perceiver resamper
            
            <img src="https://github.com/user-attachments/assets/97597589-9492-490b-9568-a812163fac12" width="500"/>
            
            - Learned latent quries = ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•˜ê¸° ì „ì— ì¿¼ë¦¬ë¥¼ í• ë‹¹í•˜ê³  í•™ìŠµí•˜ë©´ì„œ ì¿¼ë¦¬ ë¶€ë¶„ì„ í•™ìŠµí•˜ê³  fixí•´ë†“ìŒ. ì´ ê°œìˆ˜ë§Œí¼ë§Œ ì¶œë ¥ì„ ë±‰ìœ¼ë¯€ë¡œì„œ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì¶œë ¥í•˜ê²Œ í•¨.
    - vision inputì€ keyì™€ value í˜•íƒœë¡œ cross attentiom layerì— ì…ë ¥ë˜ê³ , language inputì€ query í˜•íƒœë¡œ  cross attentiom layerì— ì…ë ¥ë¨.
    - tanh gating (cross attention ì¸µ ìœ„ì— ì¡°ê·¸ë§Œí•œ ë¶€ë¶„,  FFM ìœ„ì˜ ì¡°ê·¸ë§Œí•œ ë¶€ë¶„)
        - ì´ˆê¸°í™”ë¥¼ 0ìœ¼ë¡œ í•¨. (ì´ë¥¼ í†µí•´ skip connection ì‹œ, keyì™€ queryê°€ 0ì´ ë˜ë©°, language inputë§Œ ìœ„ ë ˆì´ì–´ë¡œ ì˜¬ë¼ê°€ê²Œ ë¨.â†’ ì²˜ìŒ ì‹œì‘ì‹œ ê¸°ë³¸ Language Model í˜•íƒœë¡œ ì‹œì‘í•˜ê³  ê·¸ ì´í›„ì— visionì…ë ¥ì´ í˜ëŸ¬ ë“¤ì–´ì˜´ )
- ì´ë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ëŠ” ì ìœ¼ë©´ì„œ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” visual modelì´ ë§Œë“¤ì–´ì§.

### 2.3. LLaVA

- Flamingoì²˜ëŸ¼ ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê·¸ ì´ë¯¸ì§€ë¥¼ ê°€ì§€ê³  ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ëª¨ë¸
    
    <img src="https://github.com/user-attachments/assets/67916872-e042-4ba1-b1fa-d379772fbc6d" width="500"/>
    
    - ë¨¼ì € Pre-training ëœ LLMëª¨ë¸ê³¼ Vision Encoderë¥¼ ê°€ì§€ê³  trainableí•œ layer (projection W)í•˜ë‚˜ë§Œì„ ë°°ì¹˜í•¨.
        - projection W : vision encoderì—ì„œ ë‚˜ì˜¨ featureì„ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í° í˜•íƒœë¡œ convertingí•´ì¤Œ
        - projection Wì„ í•™ìŠµí•˜ê¸° ìœ„í•œ  visual instruction dataë¥¼ GPTë¥¼ í†µí•´ ë§Œë“¦.

### 2.4. InstructBLIP

- LLaVA, Flamingoì™€ ë¹„ìŠ·í•œ ëª¨ë¸
    
    <img src="https://github.com/user-attachments/assets/738de91e-7810-43e4-8b86-d2bdbee16f74" width="500"/>
    
    - LLaVA, Flamingoì²˜ëŸ¼ Pre-training ëœ Vision Encoderì„ ì‚¬ìš©
    - Q-Former : vision encoderì—ì„œ ë‚˜ì˜¨ featureì„ Language modelì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í° í˜•íƒœë¡œ convertingí•´ì¤Œ (Flamingoì˜ perceiver)
- Q-Former
    
    <img src="https://github.com/user-attachments/assets/ae7121a3-9d79-47a2-ab2c-77fe0d3da268" width="500"/>
    
    - image-text contrastive learning = ì´ë¯¸ì§€ featureì™€ í…ìŠ¤íŠ¸ feature ì‚¬ì´ì— ê´€ê³„ë¥¼ ì¸¡ì •
        - ì´ëŠ” ê°ê°ì˜ self-attentionì´ shareë˜ë©´ì„œ êµ¬í˜„. ì´ ë•Œ, attention maskingì„ í†µí•´ input textì™€ learned queriesê°€ ì–¼ë§Œí¼ ì„ì¼ ì§€, ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì„ì¼ ì§€ ê²°ì •.

# 3. Other visual reasoning

### 3-1. Visual programming

- ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ ì´ë¯¸ì§€ë¥¼ ì–´ë–¤ ì‹ìœ¼ë¡œ ë¶„í•´í•´ì„œ ì¬í•©ì„±í•´ì•¼ í•˜ëŠ” ì§€ ì ˆì°¨ë¥¼ ê³„íší•˜ê³  í”„ë¡œê·¸ë¨ì„ ìƒì„±í•˜ëŠ” í˜•íƒœë¡œ ë””ìì¸ë¨.
    
    <img src="https://github.com/user-attachments/assets/06be4f25-6751-4644-b9d0-f02226e509cc" width="500"/>
    

### 3-2, PaLM-E

- LLaVAì²˜ëŸ¼ ì´ë¯¸ì§€ê°€ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¯¸ì§€ë¥¼ language tokenìœ¼ë¡œ ë°”ê¿”ëŠ” ë¶€ë¶„ë„ ìˆì§€ë§Œ, í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì„œ ê·¸ ì•¡ì…˜ì„ control foundation modelë¡œ ì…ë ¥í•´ì„œ ë¡œë´‡ì„ ì œì–´í•˜ëŠ” ì‹œê·¸ë„ë¡œ convertingí•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„.

# [CV ì´ë¡ ] Generative Models

## 1. Auto-Encoder (AE)

- ì•„ì´ë””ì–´ : êµ‰ì¥íˆ ë§ì€ training dataë¥¼ NNì— ë„£ì!
    
    <img src="https://github.com/user-attachments/assets/87710a34-5d7c-41ec-b861-b32f4b5845b8" width="500"/>
    

## 2. Variational Autoencoder(VAE)

- ê°€ì • : ë°ì´í„°ëŠ” ì ì¬ ê³µê°„ì—ì„œ ìƒì„±
    
    <img src="https://github.com/user-attachments/assets/c5410140-5d00-464c-9772-ae457abd5024" width="500"/>
    
- $p_{\theta} (x|z) = p(x|g_{\theta}(z))$
    - ì´ $p_{\theta}$ëŠ” ë‹¤ìŒì²˜ëŸ¼ ìš°ë„ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì •ì˜ë¨
- xë¡œë¶€í„° zë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì€
    
    <img src="https://github.com/user-attachments/assets/5d723c88-5980-4a58-b197-807b68aab6f5" width="300"/>
    
    ê°€ ë˜ëŠ”ë°, ì´ê²ƒ ì—­ì‹œ ë¶„ëª¨ì— xì— ëŒ€í•œ ìš°ë„ê°€ ë“¤ì–´ê°€ë¯€ë¡œ ì¶”ì •í•˜ê¸° í˜ë“¦.
    
- ë”°ë¼ì„œ $p_{\theta}(z|x)$ë¥¼ ê·¼ì‚¬í•˜ëŠ” ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ $q_Ã˜(z|x)$ì„ ë‘ 
    
    <img src="https://github.com/user-attachments/assets/761462c7-73ad-49d0-b45f-77cb5e80c66c" width="500"/>
    
    - Reparameterization trickì´ë€ ê²ƒì„ ì‚¬ìš©í•¨. (Î¼, ÏƒëŠ” differentiable í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ)
        - ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ Îµì™€ Î¼, Ïƒì„ í†µí•´ zë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©. (ì„ í˜•ì„±ì´ ìˆê¸°ì— ë¯¸ë¶„ ê°€ëŠ¥!)

## 3.  Denoising Diffusion Probabilistic Models(DDPM)

- ë°ì´í„° $x_0$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë…¸ì´ì¦ˆë¥¼ ì¡°ê¸ˆì”© ì…í˜€ì„œ latentë¡œ ë§¤í•‘í•˜ê³ , ë””ë…¸ì´ì¦ˆë¥¼ í†µí•´ ë‹¤ì‹œ $x_0$ë¡œ ê°. (Markovian forward and reverse process)
    
    <img src="https://github.com/user-attachments/assets/c9d1bf08-07d2-4081-9784-223f2ae0e442" width="500"/>
    
- loss
    
    <img src="https://github.com/user-attachments/assets/7466dce4-4009-46ee-bcaa-7dd2d99b7f6d" width="500"/>
    

## 4. Latent Diffusion (a.k.a. Stable Diffusion)

<img src="https://github.com/user-attachments/assets/54a30dfc-669e-41b8-a0a5-5c1606cb9775" width="500"/>

## 5. ControlNet

- Stable Diffusionì„ foundationìœ¼ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” inputì„ í•˜ì—¬ ì‚¬ìš©.
    
    <img src="https://github.com/user-attachments/assets/ded17701-92a7-40ac-9cc1-c069899770ec" width="500"/>
    
- cë¼ëŠ” ì»¨ë””ì…˜ì´ ë“¤ì–´ì™”ì„ ë•Œ, zero-conv layerë¥¼ í†µê³¼í•´ì„œ 0ì´ ë‚˜ì˜¤ê²Œ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ inputì„ ë”í•´ì¤˜ì„œ trainable copyì— ë„˜ê²¨ì£¼ê³ , ë‹¤ì‹œ zero-conv layerë¥¼ í†µê³¼í•˜ì—¬ ì¶œë ¥ì— ë”í•¨.
    - trainable copyëŠ” networkì˜ íŒŒë¼ë¯¸í„°ë¥¼ copyí•œ ê²ƒ (input Cë„ imageê¸° ë•Œë¬¸ì— ì´ êµ¬ì¡°ì— ëŒ€í•œ ì´í•´ë¥¼ original networkë¥¼ ì´ìš©í•´ì„œ ë¹ ë¥´ê²Œ ì ìš©í•˜ê¸° ìœ„í•¨)

## 6.  LoRA : Low-RankAdaptation

- pretrained ëœ ëª¨ë¸ì´ ìˆì„ ë•Œ, ì¶”ê°€ì ì¸ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ
- ê°€ì •  : modelì´ adaptationì´ ì¼ì–´ë‚  ë•Œ, weightsì˜ ë³€í™”ë„ ì „ë¶€ ë‹¤ low dim ê³µê°„ì—ì„œ ì¼ì–´ë‚œë‹¤.
    
    <img src="https://github.com/user-attachments/assets/d80664ca-4588-48e4-9770-b1bd5cc7abb4" width="500"/>
    
    - ë”°ë¼ì„œ weightsë¥¼ ê±´ë“¤ì—¬ì„œ fine-turningí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì¶”ê°€ì ì¸ pathë¥¼ ì¤˜ì„œ ì´ë¥¼ ë”í•´ì£¼ëŠ” ëª¨ë¸ì„.
    - ì¶œë ¥ ë¶€ë¶„(B)ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ì„œ ì´ˆê¸°ì—ëŠ” ì¶œë ¥ì´ ì„œì„œíˆ ìŠ¤ë©°ë“¤ê²Œ í•¨.

## 7. Generative models í™œìš©ì‚¬ë¡€

### 7-1. Prompt-to-Prompt Image Editing

- í…ìŠ¤íŠ¸ë¥¼ í†µí•œ ì´ë¯¸ì§€ editing (ì›ë˜ ì´ë¯¸ì§€ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ editingí•˜ëŠ” ê¸°ìˆ 

### 7-2. InstructPix2Pix

- í•˜ë‚˜ì˜ ì´ë¯¸ì§€ê°€ ì£¼ì–´ì§€ê³ ,  ì–´ë–»ê²Œ ë³€ê²½í•  ì§€ text instructionì´ ì£¼ì–´ì§€ë©´, instructionì„ instructionìœ¼ë¡œ í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë°”ë¡œ ë³€ê²½.
- ì´ ë…¼ë¬¸ì€ Image Editingì„ supervised learning problemìœ¼ë¡œ ì ‘ê·¼í•¨.
    - ì¦‰, input ì¡°ê±´ê³¼ output pair datasetì„ ì˜ ë§Œë“¤ì–´ ë†“ê³ ,  ê·¸ê²ƒì„ trainingí•¨.

### 7-3. Marigold

- diffusionì„ ë‹¨ìˆœíˆ fine-turningí•´ì„œ monocular depth estimationì— ì‚¬ìš©
    
    <img src="https://github.com/user-attachments/assets/4e9cc821-0c27-49ed-bba1-3a98e8a3ca2a" width="500"/>
    
    - Stable Diffusionì—ì„œ ê¸°ì¡´ì— ì‚¬ìš©í–ˆë˜ Latent Encoderì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ Latentí™”í•˜ê³  ($z^{(x)}$, 4 ì±„ë„),  depthë„ ì´ë¯¸ì§€ë¡œ ë³´ê³  Latent Encoderì— ë„£ì–´ Latentí™”í•¨ ($z^{(d)}$, 3ì±„ë„).
    - $z^{(d)}$ì— ëŒ€í•´ì„œë§Œ denoizing diffusionì„ ì‹¤ì‹œ. ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ t ìŠ¤í…ì—ì„œ $z^{(d)}$ì— ë”í•˜ì—¬ perturbationí•¨. (4 ì±„ë„ì´ ë¨.)
    - ì´ë¥¼ ì´ë¯¸ì§€ì˜ Latentì™€ concatí•˜ê³ , (8 ì±„ë„ì´ ë¨.) u-netì— ë„£ëŠ”ë°, ì´ë•Œ, u-netì´ 8ì±„ë„ì„ inputìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ zero-conv êµ¬ì¡° ë“±ì„ ì‚¬ìš©í•¨.
    - ì´ë¥¼ í†µí•´ U-netì´ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ëŠ” í˜•íƒœë¡œ denoizing diffusionì„ trainingí•¨. (U-netì´ fine-turning ë¨.)

# [CV ì´ë¡ ] 3D Understanding

## 1.  3D reconstruction

### 1-1. NeRF

- x, y, z í¬ì¸íŠ¸í•˜ê³  ê·¸ í¬ì¸íŠ¸ë¥¼ ë°”ë¼ë³´ëŠ” ë°©í–¥ì¸ viewing directionì„ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´, ê·¸ í¬ì¸íŠ¸ì— ëŒ€í•œ ìƒ‰ê¹”ê³¼ íˆ¬ëª…ë„ë¥¼ ì¶œë ¥.
    
    <img src="https://github.com/user-attachments/assets/fdd5debf-e6e1-47f1-9bf8-282c1c704dab" width="500"/>
    
- ëª¨ë¸ ì¶œë ¥ê³¼ ì´ë¯¸ì§€ì˜ í‘œí˜„ í˜•íƒœê°€ í˜¸í™˜ë˜ì§€ ì•ŠìŒ (ì…ë ¥ì´ 3D í¬ì¸íŠ¸ì— ëŒ€í•œ ìƒ‰ê¹”ì„.)
- ë”°ë¼ì„œ volumetric renderingì´ë¼ëŠ” í…Œí¬ë‹‰ì„ í™œìš©
    - ì»´í“¨í„° ê·¸ë˜í”½ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë Œë”ë§ ë°©ì •ì‹ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë Œë”ë§í•˜ë„ë¡ ê³„ì‚°
        
        <img src="https://github.com/user-attachments/assets/7d2b4b04-bd06-4899-9f20-41e55202f45a" width="500"/>
        
        - í•˜ë‚˜ì˜ í”½ì…€ì€ ì¹´ë©”ë¼ ì„¼í„°ë¡œ ë“¤ì–´ì˜¤ëŠ” ì§ì„  ìƒì˜ ëª¨ë“  ê°ì²´ì˜ ìƒ‰ê¹”ì˜ ëˆ„ì ìœ¼ë¡œ ì´ë¤„ì§€ëŠ”ë°, (ì¹´ë©”ë¼ ì„¼ì„œ) ì´ë¥¼ ëª¨ë¸ë§í•œ ì‹ì„.
        - $Ïƒ(r(t))$ = density. ì´ê²ƒì´ 0ì´ë©´, íˆ¬ëª…í•˜ê²Œ ì·¨ê¸‰ (ëˆ„ì í•˜ì§€ ì•ŠìŒ)í•˜ê³ , 1ì´ë©´ ì™„ë²½í•˜ê²Œ ë¶ˆíˆ¬ëª…í•œ ì…ìì´ë¯€ë¡œ ê·¸ê²ƒì„ ê³ ë ¤í•¨.
        - $c(r)$ = ëœë”ë§ëœ plane ìƒì—ì„œì˜ í•œ í”½ì…€
        - $T(t)$ = ë¬¼ì²´ ë’¤ ìª½ì˜ ìƒ‰ê¹”ê¹Œì§€ ëˆ„ì ë˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•¨. ê°€ë ¤ì§„ ìˆœê°„ ì „ê¹Œì§€ 1ì„ ê°€ì§€ë‹¤ê°€ ê°€ë ¤ì§„ ë¬¼ì²´ë¥¼ ë§Œë‚˜ë©´ 0ì´ ë¨.

### 1-2. 3D Gaussian Splatting (3DGS)

- 3D Gaussian Splattingì€ NeRFì— ë¹„í•´ 10ë°° ì´ìƒì˜ ë„ì•½ì„ ë³´ì—¬ì£¼ë©°, ê°€ì¥ í™œë°œí•˜ê²Œ ì‚¬ìš©ë¨.
    
    <img src="https://github.com/user-attachments/assets/e16afe5c-6747-4e84-8b88-5ec8e5a791f6" width="500"/>
    
- NeRFì—ì„œëŠ” ì „ì²´ ì¥ë©´ì„ í•˜ë‚˜ì˜ NNë¡œ í‘œí˜„í–ˆë‹¤ë©´, 3D Gaussian Splattingì€ 3Dì—ì„œ êµ­ë¶€ì ì¸ ê³µê°„ì„ í•˜ë‚˜ì˜ ê°€ìš°ì‹œì•ˆìœ¼ë¡œ í‘œí˜„
- ê° 3D ê°€ìš°ì‹œì•ˆì€ Mean Î¼ (i.e., x, y, z), Covaraince Î£, Opacity Ïƒ, Color parameters; (R, G, B) (or spherical harmonics (SH))ë¡œ ì´ë£¨ì–´ì§.
    - meaní•˜ê³  covarianceì€ 3D ê°€ìš°ì‹œì•ˆ ìì²´ë¥¼ í‘œí˜„í•˜ê³ , ì´ ê°€ìš°ì‹œì•ˆì— ëŒ€í•œ Opacity, Color parametersì„ ê°€ì§.
    - covarianceì€ positive semi-definite matrixì´ë¯€ë¡œ ì´ íŠ¹ì„±ì„ ìœ ì§€í•˜ë©´ì„œ í•™ìŠµí•˜ëŠ” ê²Œ ì‰½ì§€ ì•Šì•„  Rotation Matrix $R$ ê³¼ diagonalí•œ scaling matrix $S$ë¡œ symmetricí•˜ê²Œ ê²°í•©í•´ì„œ ëŒ€ì‹  í‘œí˜„

        <img src="https://github.com/user-attachments/assets/8bdcafdd-abcf-4547-989d-81dd43cfcfa0" width="200"/>
    

## 2. 3D generation

### 2-1. Mesh R-CNN

- Mesh R-CNN=Faster R-CNN+3D surface regression branch
    
    <img src="https://github.com/user-attachments/assets/68b0bb99-2361-4ee4-96ab-dcc52d2ec8e8" width="500"/>
    
    - ì´ë¯¸ì§€ê°€ ì£¼ì–´ì§€ë©´ í•œì •ëœ classì— ëŒ€í•´ì„œ mesh outputì´ ë‚˜ì˜¤ëŠ” êµ¬ì¡°ì´ë¯€ë¡œ ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ì— ëŒ€ì‘í•˜ëŠ” 3Dê°€ ì˜ ì •ë ¬ëœ í° ë°ì´í„°ì…‹ì´ í•„ìš”í•¨.

### 2-2. DreamFusion

- í…ìŠ¤íŠ¸ì™€ 3D pairê°€ ì—†ì´ë„ ì¢‹ì€ í€„ë¦¬í‹°ì˜ 3Dë¥¼ ìƒì„±í•˜ëŠ” zero-shot ë°©ë²•
- Diffusionê°™ì€ ìƒì„± ëª¨ë¸ì€ ì•„ë‹ˆê³  trainingì—†ì´ ìµœì í™” í˜•íƒœë¡œ 3Dë¥¼ êµ¬í˜„
    
    <img src="https://github.com/user-attachments/assets/a7009752-2c56-4560-acac-44e4497ce7a0" width="500"/>
    
- Score Distillation Sampling (SDS) loss
    - ë Œë”ë§ëœ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆë¥¼ ì²¨ê°€í•˜ì—¬ diffusion modelì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê³ , ì´ ì´ë¯¸ì§€ê°€ fittingë˜ì—ˆìœ¼ë©´ í•˜ëŠ” textë„ ë„£ì–´ì¤Œ
    - ì´ ë…¸ì´ì¦ˆ ë‚€ ì´ë¯¸ì§€ë¥¼ textë¥¼ ë”°ë¥´ëŠ” í˜•íƒœë¡œ denoizingí•˜ë„ë¡ í•¨ (í˜„ì¬ ìƒíƒœì—ì„œ ì–´ë–»ê²Œ ë°”ë€Œì–´ì•¼ textë¥¼ ë”°ë¥´ëŠ” í˜•íƒœë¡œ ë˜ëŠ” ì§€ ë°©í–¥ì„ ì•Œë ¤ì£¼ëŠ” ê²ƒ)
    - U-Net êµ¬ì¡°ì— ëŒ€í•´ì„œëŠ” ì—­ì „íŒŒë¥¼ ì§„í–‰í•˜ì§€ ì•ŠìŒ.  (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê²Œ ë˜ë¯€ë¡œ)

### 2-3. Paint-it

- 3Dê°€ ì•„ë‹Œ 3D í…ìŠ¤ì³ë¥¼ ìƒì„±
- SDS lossì˜ ê·¸ë ˆì´ë””ì–¸íŠ¸ê°€ ë…¸ì´ì¦ˆí•´ì„œ ì¢‹ì€ 3D ìƒì„±ì— ë°©í•´ê°€ ë  ìˆ˜ ìˆìŒ. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ SDS lossë¥¼ ì‚¬ìš©í•˜ë©´ì„œë„ í…ìŠ¤ì³ ë§µì„ convNetìœ¼ë¡œ parameterizationí•¨.
    - ì¦‰, ì…ë ¥ì€ ëœë¤ ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•´ì„œ fixí•˜ê³  convNetë„ ëœë¤ ì´ˆê¸°í™”ë¥¼ í•¨. ëŒ€ì‹  outputì´ í…ìŠ¤ì³ ë§µì´ ë  ìˆ˜ ìˆë„ë¡ resolutionì„ ë§ì¶°ë†“ìŒ.
    - ì²˜ìŒì—ëŠ” ëœë¤í•œ ê°’ì´ ë‚˜ì˜¤ì§€ë§Œ,  ì°¨ì¦˜ í…ìŠ¤ì³ ë§µì„ í‘œí˜„í•˜ëŠ” CNN weightê°€ ìµœì í™” ë¨.

# [CV ì´ë¡ ] 10. 3D Human

## 1. SMPL : Body model

- ì‹ ì²´ ëª¨ë¸ ğ‘€(âˆ™)ì€ ì‘ì€ ìˆ˜ì˜ í¬ì¦ˆ, ëª¨ì–‘ ë° ê¸°íƒ€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì·¨í•˜ê³ , 3D meshì„ ë°˜í™˜í•¨.
    
    <img src="https://github.com/user-attachments/assets/61d8bddc-2e6e-41b3-ad89-1e98e7a6794a" width="500"/>
    
- SMPLì€ meshë¥¼ ì•½ 7,000ê°œì˜ 3D Vertexë¡œ í‘œí˜„í•˜ê³  ìˆì–´ 21,000ê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ (x,y,z 3ì°¨ì› ì¸ë“¯) ì‚¬ëŒì„ í‘œí˜„í•¨.
- body modelì€ ì²´í˜•ê³¼ ê´€ë ¨ëœ poseì™€ identity íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ëª¨ë¸ë§ ë¨. ì´ ë‘ ìš”ì†Œë¥¼ ë…ë¦½ ìš”ì†Œë¡œ ìƒê°í•´ì„œ í•™ìŠµì´ë‚˜ ì¶”ë¡ ì„ ë” ê°„ë‹¨í•˜ê²Œ í•¨.
- SMPL parameterization
    
    <img src="https://github.com/user-attachments/assets/fed0d10f-7d80-4a8a-a277-cc1a01e51a01" width="500"/>
    
    - $T$ :  Shape training set (Template) = CAESAR ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì„œ ì •êµí•œ í¬ì¦ˆë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ë¥¼ íšë“. ì´ ë°ì´í„°ë“¤ì˜ vertex ê°’ë“¤ì„ ê° ìœ„ì¹˜ì— ë§ê²Œ í‰ê· ë‚´ì„œ Template meshì— ëŒ€ì‘í•˜ëŠ” í‰ê·  meshë¥¼ ë§Œë“¦
    - $S$ : Shape blend shape matrix = í‰ê·  ëŒ€ë¹„ ë³€í™”ë¥¼ ì˜ ëª¨ë¸ë§ í•˜ê¸° ìœ„í•´ì„œ meshë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ê³  í…œí”Œë¦¿ meshë¥¼ í‰ê· ìœ¼ë¡œ ê³ ë ¤ë¥¼ í•´ì„œ ê° meshì—ì„œ ë¹¼ì£¼ê³  ì´í›„ PCAë¥¼ ìˆ˜í–‰í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•¨. ($U$) Uë¥¼ shape parametersì™€ì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ ê°ê°ì˜ í¸ì°¨ ë²¡í„°ë¥¼ í‘œí˜„
    - $W$ : Blend weights matrix = Skinningì„ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ ì›€ì§ì„ì„ ë§Œë“¦.
        - Skinning : Rest pose vertices (í…œí”Œë¦¿ mesh), Joint locations, Weights, Pose parametersê°€ ì£¼ì–´ì§€ë©´, ê° í¬ì¦ˆì— ë§ëŠ” 3D meshë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒ
    - $p$ : Pose blend shape matrix = ì‹¤ì œ í”¼ë¶€ëŠ” ìì„¸ì— ë”°ë¼ ë°€ë¦¬ê¸°ì— ëª¨ë¸ë§ì— í•œê³„ê°€ ìˆdj ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ í–‰ë ¬. Blend shapeì„ rest poseì— ì„ í˜• ê²°í•©ì„ í†µí•´ ë”í•˜ë¯€ë¡œì„œ ìì„¸ë¥¼ ë³´ì™„
    - $J$ : Joint regressor matrix = ì¡°ì¸íŠ¸ í¬ì¸íŠ¸ë“¤ì„ ì •ì˜í•˜ëŠ” í–‰ë ¬. ë¯¸ë¦¬ ì •ì˜í•´ë‘ê³  ì‚¬ìš©.
- ìµœì¢… SMPL parameterization
    
    <img src="https://github.com/user-attachments/assets/18b31acb-4bcc-4ca0-997a-5e3e066fd6c3" width="500"/>
    

## 2. SMPLify

- 2D Joint featureë¥¼ ì¶”ì¶œí•˜ê³ (Bottom up) 3D ë°”ë”” ëª¨ë¸ì„ ì¹´ë©”ë¼ ë©”íŠ¸ë¦­ìŠ¤ë¥¼ í†µí•´ì„œ projectioní•˜ì—¬ 2D ë„ë©”ì¸ìœ¼ë¡œ ì²˜ë¦¬í•œ ë‹¤ìŒì— ì•ì—ì„œ êµ¬í•œ 2D Jointì™€ projectionëœ ëª¨ë¸ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ì—¬  (Top down) ìµœì í™”ì— ì‚¬ìš©
    
    <img src="https://github.com/user-attachments/assets/cf2d3c59-e2a1-42ac-bcbc-d1b99264b96b" width="500"/>
    
    - ì…ë ¥ìœ¼ë¡œëŠ” ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” 2D Point ì¶”ì •ì„ ì‚¬ìš©í•´ì„œ 2D Point jointë¥¼ ë¯¸ë¦¬ êµ¬í•´ ì…ë ¥ìœ¼ë¡œ ë„£ìŒ. ì¹´ë©”ë¼ í–‰ë ¬ì€ í¬ì¦ˆ ì „ì²´ì— íšŒì „ì„ ê³ ë ¤í•œ ê°„ë‹¨í•œ 2*3 í–‰ë ¬ì„ ì‚¬ìš©
- Depth AmbiguityëŠ” Pose and Shape Priorì„ í†µí•´ í•´ê²°
- InterpenetrationsëŠ” Approx. surface with capsules and penalize intersectionsì„ í†µí•´ í•´ê²°
- Objective Function
    
    <img src="https://github.com/user-attachments/assets/91f671fc-34cc-449b-a683-bdca742ff020" width="500"/>
    
    - data term
        - 2D Joint estimationì„ í†µí•´ì„œ êµ¬í•œ ê°’($J_{est, i}$)ê³¼ ì´ˆê¸°í™” ê°’ì¸ $\beta$ë¡œë¶€í„° ì¶”ì •ëœ ì¡°ì¸íŠ¸ ê°’ $J(\beta_i)$ê°€ ì£¼ì–´ì§€ë©´, ì´ë¥¼ $R_{\theta} (*)$ì„ ì ìš©í•˜ì—¬ 3D ì¢Œí‘œê°€ ì •í•´ì§€ê²Œ ë˜ê³ , ì¹´ë©”ë¼ ë©”íŠ¸ë¦­ìŠ¤ë¥¼ í†µí•´ projectioní•˜ì—¬ 2Dë¡œ ë§Œë“¦.
        - ì´ 2Dë¡œ Projectionëœ Jointí•˜ê³  ì¸¡ì •ëœ $J_{est, i}$ì™€ ì°¨ì´ë¥¼ ê³„ì‚°. ÏëŠ” $L_2$, $L_1$ê³¼ ê°™ì€ normì„ ì§€ì¹­
        - $w_i$ : ì¼ë¶€ jointê°€ ê°€ë ¤ì ¸ì„œ ë³´ì´ì§€ ì•Šì„ ë•Œ ì¡°ì¸íŠ¸ì— ëŒ€í•´ì„œ lossë¥¼ ì¸¡ì •í•˜ì§€ ì•Šê²Œ í•˜ëŠ” ê°’. (ë§ˆìŠ¤í¬ ì—­í• )
    - Prior term - ì¡°ì¸íŠ¸ê°€ ë¶€ìì—°ìŠ¤ëŸ½ê²Œ êº¾ì´ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” term
        - prior = í˜„ì¬ ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê·¸ê²ƒì´ ë°œìƒí•˜ê¸° ì‰¬ìš´ ê²½ìš°ì¸ì§€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì¸ì§€ íŒë‹¨í•˜ëŠ” ê¸°ì¤€
        - unnatural joint bending, prior on pose, prior on shape, prior on interpenetration ë“±ì˜ termì´ ìˆìŒ. (ì„¸ë¶€ ë‚´ìš©ì€ ìƒëµ)

## 3. SPIN

- SMPLì„ NNê³¼ ì—°ë™í•˜ì—¬ ë” ê°•ê±´í•˜ê³  íš¨ìœ¨ì ì¸ ë°©ë²• ì œì‹œ
    
    <img src="https://github.com/user-attachments/assets/8e990c2c-8b27-4731-af2c-e30c1feb88fd" width="500"/>

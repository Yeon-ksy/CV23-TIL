# 8/30~9/1 WIL âœï¸

## íƒœìš± [[Github](https://github.com/K-ple)]

### ViT(Vision Transformer)

![img.png](img.png)
Self-attention ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì´ìš©í•œ Trnsformerë¥¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ê°€ ì•„ë‹Œ Computer Vision ë¶„ì•¼ì— ì ìš©í•œ ë„¤íŠ¸ì›Œí¬ì´ë‹¤.

#### íŠ¹ì§•

- Transformerì˜ Encoderë¶€ë¶„(Self-attention)ì„ ê·¸ëŒ€ë¡œ ì‘ìš©
- Vision Taskì—ì„œ CNNì„ ì´ìš©í•˜ì§€ ì•Šê³  ì¶©ë¶„í•œ í¼í¬ë¨¼ìŠ¤ë¥¼ ë‚¼ ìˆ˜ ìˆìŒ

### NLP history from transfomer

- RNNs's problem : ìˆœë°©í–¥ í†µê³¼ ì¤‘ ì •ë³´ê°€ ì†ì‹¤
- Bi-directionalRNNs's problem : ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ íŒ¨ìŠ¤ëŠ” ëª¨ë‘ ì–‘ë°©í–¥ ì •ë³´ë¥¼ ì˜ë¯¸í•˜ê²Œ ë¨

- Transformers : RNNsì˜ ì¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬ì˜ ë¬¸ì œì ê³¼ ì…ë ¥ ë¬¸ì¥ì„ ì „ì²´ì ìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬í•œë‹¤ëŠ” BRNNsì˜ ë¬¸ì œì ì„ í•´ê²°í•¨

## ìƒìœ  [[Github](https://github.com/dhfpswlqkd)]

### DERT

![alt text](images/DERT_architecture.png)

#### 1. CNN backbone

ì´ë¯¸ì§€ë¥¼ CNN backboneì— ì…ë ¥í•˜ì—¬ feature mapì„ ì¶œë ¥ìœ¼ë¡œ ì–»ëŠ”ë‹¤. `(C, H, W)`

#### 2. Positional Encoding

feature mapì„ 1x1 convolutionì„ í†µí•´ d ì°¨ì›ìœ¼ë¡œ ê°ì†Œì‹œí‚¨ í›„ `(d, HW)`ë¡œ ë³€í™˜í•œë‹¤. (HWê°€ ì‹œí€¸ìŠ¤ ìˆ˜ë¼ê³  ìƒê°í•˜ë©´ ë  ë“¯)
transformerì™€ ê°™ì´ position encodingì„ ìˆ˜í–‰í•´ì¤€ë‹¤. (~~ì‚¬ì‹¤ ì¡°ê¸ˆ ë‹¤ë¦„~~)

#### 3. Transformer (í‹€ë¦° ë¶€ë¶„ ìˆì„ìˆ˜ë„ ìˆì–´ìš”)

NLPì˜ Transformerê³¼ ë‹¤ë¥´ê²Œ Decoderì—ì„œ object queries`(N, d)`ë¥¼ ì…ë ¥í•œë‹¤. object queriesëŠ” objectì˜ ë¼ë²¨ê³¼ ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡í•œë‹¤. ë˜í•œ Decoderì˜ ê²°ê³¼ë¡œ ë‘ ê°œì˜ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.
Decoderì—ì„œ í¬ì§€ì…˜ ì„ë² ë”©ì€ self-Attentionë§ˆë‹¤ ë”í•´ì¤€ë‹¤. í¬ì§€ì…˜ ì„ë² ë”© ë˜í•œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.

#### 4. Prediction heads

FFN(ê·¸ëƒ¥ Linear)ì„ í†µí•´ Class`(N, í´ë˜ìŠ¤ ìˆ˜+1)` ì˜ˆì¸¡ê³¼ Bounding Box`(N, 4)` ì˜ˆì¸¡ì„ í•œë‹¤.

#### 5. Match

ì œì•ˆëœ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ìµœì ì˜ ë§¤ì¹­ì„ ì°¾ëŠ”ë‹¤.

#### ì†ì‹¤í•¨ìˆ˜

class Lossì™€ Box Lossë¡œ ë‚˜ëˆ„ì–´ ì§„ë‹¤. class LossëŠ” í‰ë²”í•œê±° ê°™ë‹¤.
Box LossëŠ” L1 lossì™€ GIoUë¥¼ í™œìš©í•œë‹¤.
![alt text](images/GIoU.png)

## ì§€í˜„ [[Github](https://github.com/jihyun-0611)]

### Introduction to Computer Vision

1. ë¨¸ì‹ ëŸ¬ë‹ì€ feature extractionê³¼ classificationì´ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤.
2. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë‹¬ë¦¬ ë”¥ëŸ¬ë‹ì€ feature extraction ê³¼ classificationì„ ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•œë‹¤.
3. Knowledge distillation

   <img src="https://blog.roboflow.com/content/images/size/w1000/2023/05/data-src-image-fe4b322a-6c99-4803-9b1a-e7a038f0eb32.png" width="500" height="150"/>

4. Image â‡’ projection of the 3D world onto an 2D image plane
5. **Computer vision == Visual perception & intelligence**
   - teach a machine â€œhow to see and imagineâ€!
   - computer vision includes understanding human visual perception capability!

### CNN

1. CNN architectures

   1. LeNet-5
   2. AlextNet : Simple CNN architecture
      - ê°„ë‹¨í•œ ì—°ì‚°, ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
      - ë‚®ì€ ì •í™•ë„
   3. VGGnet : simple with 3x3 convolutions
      - ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©, ë¬´ê±°ìš´ ì—°ì‚°
   4. GoogLeNet
   5. ResNet : deeper layers with residual blocks
      - Moderate efficiency (depending on the model)
   6. Beyond ResNet

      : Going deeper with convolutions

      : VGGnet and ResNet are typically used as a backbone model for many tasks

2. Vision Transformers (ViT)

   : Apply a standard transformer directly to images
   <img src="images/ViT.png" alt="alt text" width="400"/>

   â†’ Overall architecture

   - Split an image into fixed-size patches
   - Linearly embed each patch
   - Add positional embeddings
   - Transformer Encoder
   - Feed a sequence of vectors to a standard transformer encoder
   - Classification token

   ***

   1. Scaling law (not for all model, but tranformer also ViT)

      : If there is large amount of data

      â†’ the model size increases, the better performance

      â†’ the more data is provided, the better performance

   2. Advanced ViTs

      â†’ Swin Transformer

      â†’ masked autoencoders(MAE)

      â†’ DINO

## ìœ¤ì„œ [[Github](https://github.com/myooooon)]

### [CV ì´ë¡ ]

### 2. CNNë¶€í„° ViTê¹Œì§€

#### CNN (Convolutional Neural Networks)

- CNNì€ fully **locally** connected neural networkë¡œ local featureë¥¼ í•™ìŠµí•˜ê³  parameterë¥¼ ê³µìœ í•˜ì—¬ fully connected neural networkë³´ë‹¤ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ íš¨ê³¼ì ì¸ ì´ë¯¸ì§€ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.
  <img src="images/CNN_comparison.png" alt="alt text" width="600"/>

- CNNì€ ë§ì€ CV taskì˜ backboneìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.  
   ex) Image-level classification, Classification+Regression, Pixel-level classification

#### Receptive field in CNN

- íŠ¹ì • CNN featureê°€ inputì˜ ì–´ë–¤ ì˜ì—­ìœ¼ë¡œë¶€í„° ê³„ì‚°ë˜ì–´ì˜¨ ê±´ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
- Receptive field size ê³„ì‚° ë°©ë²•
  - K x K conv filter(stride 1), P x P pooling layer(stride 2)  
     : (P + K -1) x (P + K - 1)  
    <img src="images/Receptive_field.png" alt="alt text" width="450"/>

#### ViT (Vision Transformers)

- NLPì—ì„œ transformer ëª¨ë¸ì˜ scaling successì— ì˜í–¥ì„ ë°›ì•„ ë§Œë“¤ì–´ì§„ ëª¨ë¸ë¡œ standard transformerë¥¼ ì´ë¯¸ì§€ì— ì§ì ‘ ì ìš©í•œë‹¤.
- ViTëŠ” decoderì—†ì´ encoderë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.

- Overall architecture (ê¸°ë³¸ ê³¼ì œ 1ì—ì„œ ì‹¤ìŠµ)
  1.  ì´ë¯¸ì§€ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ patchë“¤ë¡œ ë‚˜ëˆˆë‹¤.
  2.  ê° patchë¥¼ embeddingí•˜ê³ , ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•œ ë³„ë„ì˜ classification tokenì„ ê²°í•©í•œë‹¤.
  3.  ê³µê°„ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ embedding ë²¡í„°ì— positional embedding ë²¡í„°ë¥¼ ë”í•œë‹¤.
  4.  Transformer encoderì— ë„£ì–´ output ë²¡í„°ë¥¼ ì–»ëŠ”ë‹¤.
  5.  Classification tokenì˜ ê°’ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œë‹¤.  
      <img src="images/ViT.png" alt="alt text" width="450"/>

### 3. CNN ì‹œê°í™”ì™€ ë°ì´í„° ì¦ê°•

#### CNN ì‹œê°í™”

- CNN ëª¨ë¸ ë‚´ë¶€ëŠ” ì´í•´í•˜ê¸° ì–´ë ¤ìš´ black boxë¼ì„œ ì™œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€, ì–´ë–»ê²Œ ê°œì„ í•´ì•¼í•˜ëŠ”ì§€ íŒŒì•…í•˜ê¸° ì–´ë µë‹¤. ëª¨ë¸ì˜ í–‰ë™ì„ ë¶„ì„í•˜ê³  ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë§ˆì¹˜ debugging toolì²˜ëŸ¼ visualization toolì„ ì´ìš©í•œë‹¤.

#### Data augmentation

- Training datasetì€ real dataì˜ ì¼ë¶€ë§Œì„ ë°˜ì˜í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œì™€ëŠ” ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤. ì´ ì°¨ì´ë¥¼ ì¤„ì´ê³  ë” ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ì±„ìš°ê¸° ìœ„í•´ data augmentationì„ ì§„í–‰í•œë‹¤.  
   ex) Brightness, Rotate, Crop, Affline, CutMix ...

- RandAugment : ì—¬ëŸ¬ augmentation methods ì¤‘ì— ìµœì ì˜ method sequenceì„ ì°¾ê¸° ìœ„í•´ ìë™ìœ¼ë¡œ augmentation ì‹¤í—˜ì„ ì§„í–‰í•˜ëŠ” ê²ƒ

- Copy-Paste : í•œ ì´ë¯¸ì§€ì˜ segmentë¥¼ ë‹¤ë¥¸ ì´ë¯¸ì§€ì™€ í•©ì„±í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•

- Video Motion Magnification : ë³´ê¸° ì–´ë ¤ìš´ ì‘ì€ motionì„ ì¦í­ì‹œì¼œ ëˆˆì— ì˜ ë„ë„ë¡ ë§Œë“œëŠ” ê¸°ë²•
  - Copy-pasteì™€ ê²°í•©í•˜ì—¬ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í•©ì„± ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤.

## ì„¸ì—° [[Github](https://github.com/Yeon-ksy)] [[Velog](https://velog.io/@yeon-ksy/)]

### [CV ì´ë¡ ] 2. CNN
#### Brief history
![Screenshot from 2024-08-29 17-09-34](https://github.com/user-attachments/assets/a9af567c-609e-467f-bdf4-76fcdaec32d2)
- LeNet-5

   <img src="https://github.com/user-attachments/assets/909a1317-df87-4c50-b10c-4881f895eefb" width="500"/>
   
   - Overall architecture : Conv- Pool- Conv- Pool- FCÂ­ - FC
   - Convolution : 5x5 filters with stride 1
   - Pooling : 2x2 maxpooling with stride 2
- AlexNet

   <img src="https://github.com/user-attachments/assets/fa46edb3-9ea8-42ef-ac9b-2899925615cd" width="500"/>
   
   - Overall architecture : Conv- Pool- LRN- Conv- Pool- LRN- Conv- Conv- Conv- Pool- FC- FC- FC
   - LeNet-5ì™€ ë‹¤ë¥¸ ì  : ëª¨ë¸ì´ ì»¤ì§, ReLU, Dropout ì‚¬ìš©.
   - ì´ ëª¨ë¸ì„ í†µí•´ Receptive field ì‚¬ì´ì¦ˆì˜ ì¤‘ìš”ì„±ì´ ì»¤ì§.
      - Receptive field : í•œ í”½ì…€ì— í•´ë‹¹í•˜ëŠ” íŠ¹ì§•ì— ëŒ€í•´ì„œ ì–´ëŠ ì •ë„ì˜ ì…ë ¥ ë²”ìœ„ë¡œë¶€í„° ì •ë³´ê°€ ì˜¤ëŠ” ì§€ë¥¼ ì˜ë¯¸
      <img src="https://github.com/user-attachments/assets/638a7bc8-3192-4b8f-bed1-b7dc4df3e37d" width="300"/>
      
- VGGNet

   <img src="https://github.com/user-attachments/assets/407e2332-43ca-4390-b5dd-decf740af991" width="500"/>
   
   - Receptive fieldë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í‚¤ìš°ëŠ” ë°©ë²•ì„ ê³ ì•ˆ â†’ ë ˆì´ì–´ë¥¼ ë” ê¹Šê²Œ ìŒ“ìŒ.
   - local response normalizationì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
   - ì˜¤ì§ 3 Ã— 3 í•©ì„±ê³± í•„í„° ë¸”ë¡, 2 Ã— 2 Max Poolë§Œ ì‚¬ìš©

- VGGNet

   <img src="https://github.com/user-attachments/assets/12e6afd1-954c-4c63-a42c-cae0ae291c9d" width="500"/>

   - Residual blockì„ í†µí•´ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ë” ê¹Šì€ ë ˆì´ì–´ë¥¼ ìŒ“ì„ ìˆ˜ ìˆê²Œ í•¨

      <img src="https://github.com/user-attachments/assets/f18b2ab4-08f3-4401-ab2c-edeef5bfdb5f" width="300"/>

   - He ì´ˆê¸°í™” ë° ì‹œì‘ ë¶€ë¶„ì— í•©ì„±ê³± ë ˆì´ì–´ ì‚¬ìš©
   - ëª¨ë“  Residual blockì—ëŠ” ë‘ ê°œì˜ 3 x 3 í•©ì„±ê³± ë ˆì´ì–´ê°€ ìˆìœ¼ë©°, ëª¨ë“  cov ë ˆì´ì–´ ë‹¤ìŒì—ëŠ” ë°°ì¹˜ ì •ê·œí™”
   - í’€ë§ ë ˆì´ì–´ ëŒ€ì‹  í•„í„° ìˆ˜ë¥¼ ë‘ ë°°ë¡œ ëŠ˜ë¦¬ê³  ìŠ¤íŠ¸ë¼ì´ë“œ 2ë¡œ ëŒ€ì‹ í•˜ì—¬ featureì˜ ì±„ë„ì„ 2ë°° ëŠ˜ë ¤ì£¼ëŠ” ì‹ìœ¼ë¡œ ì •ë³´ëŸ‰ì„ ìœ ì§€
   - ì¶œë ¥ í´ë˜ìŠ¤ì— ëŒ€í•´ ë‹¨ì¼ FC ë ˆì´ì–´ë§Œ ì‚¬ìš©

#### Vision Transformers (ViT)
   <img src="https://github.com/user-attachments/assets/c0b9926f-4de3-4eab-82cf-a0a17a93fc8f" width="500"/>
   
   - Transformerì˜ ì¸ì½”ë”ë§Œ ì‚¬ìš©.
   - ì´ë¯¸ì§€ë¥¼ ê³ ì •ëœ patches ì‚¬ì´ì¦ˆë¡œ ë¶„í• í•¨.
      - $x \in \reals^{H * W * C} â†’ x_p \in \reals^{N * (P^2 * C)}$
      - (H, W) : resolution of the original image
      - C : the number of channels
      - (P, P) : resolution of each image patch
      - N = $HW / P^2$, :the number of patches
   - Position Encoding
      - 1D Positional Encodingì„ ì‚¬ìš©.
      - '*' í† í°ì€ Classification token
   - Transformer
      - íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”©ì„ ì‚¬ìš©.
      - íŒ¨ì¹˜ ê°œìˆ˜ë§Œí¼ ì¶œë ¥ í† í°ì´ ë‚˜ì˜¤ê²Œ ë˜ì§€ë§Œ, ì¶œë ¥ í† í°ì€ ë²„ë¦¼. ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.

#### Additional ViTs
- Swin Transformer

   <img src="https://github.com/user-attachments/assets/4ad96b97-cec4-494a-a495-f2ad3aedb3fe" width="500"/>

   - ì…ë ¥ì€ ê³ í•´ìƒë„ íŒ¨ì¹˜ë¡œ êµ¬ì„±í•˜ì§€ë§Œ, ë¸”ë¡ì„ ë‚˜ëˆ  ê·¸ê²ƒë§Œ Attentioní•˜ëŠ” êµ¬ì¡°

      <img src="https://github.com/user-attachments/assets/71c57767-0541-4dea-8398-2addd5ece591" width="400"/>
   - ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ë³‘í•©í•˜ì—¬ ê³„ì¸µì  íŠ¹ì§• ë§µì„ ìƒì„±
   - ê° ë¡œì»¬ ìœˆë„ìš° (ë¹¨ê°• ìƒì) ë‚´ì—ì„œë§Œ self-attentionì„ ê³„ì‚°í•˜ë¯€ë¡œ ê³„ì‚° ë³µì¡ë„ê°€ ì„ í˜•ì ì„. 
   - ì¶œë ¥ ì¸µì€ classificationì— ë§ê²Œ, ì¤‘ê°„ ì¸µì€ segmentation, detectionì— ë§ê²Œ êµ¬ì„±í•¨.
      <img src="https://github.com/user-attachments/assets/2e7413e3-4812-48e4-9140-550312595205" width="400"/>

      - ë°•ìŠ¤ë¼ë¦¬ì˜ ì •ë³´ë¥¼ ì„ê¸° ìœ„í•´ ë‹¤ìŒ ë ˆì´ì–´ì—ì„œëŠ” ìœˆë„ìš°ì˜ ì •ì˜ë¥¼ shift
- Masked Autoencoders(MAE)
   <img src="https://github.com/user-attachments/assets/75cc5b36-1748-4a41-90e4-60e065954901" width="500"/>
   - ì…ë ¥ íŒ¨ì¹˜ë¥¼ maskedí•˜ê³ , ì†Œìˆ˜ì˜ ë°ì´í„°ë§Œ í™œìš©í•´ì„œ Trainingí•˜ê³ , ê·¸ ì´í›„, Mask tokensì„ ë„ì…í•¨. ì´ë¥¼ í†µí•´ì„œ ì›ë˜ ì´ë¯¸ì§€ë¥¼ ë³µì›.

- DINO

   <img src="https://github.com/user-attachments/assets/8809f064-e484-4274-af11-a07bb71f1ccc" width="200"/>

### [CV ì´ë¡ ] 4. Segmentation & Detection
#### Segmentation ì¢…ë¥˜
   - Semantic segmentation = ê°™ì€ ê°ì²´ê°€ ì—¬ëŸ¬ ê°œë¼ë„ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ.
   - instance segmentation = ê°™ì€ ê°ì²´ë¼ë„ êµ¬ë¶„í•¨.
   - Panoptic segmentation = ë°°ê²½ ë¶€ë¶„ ë“± ëª¨ë“  Pixelì„ ë‹¤ segmentationí•¨ (Semantic + instance)
![Screenshot from 2024-08-29 17-09-34](https://github.com/user-attachments/assets/d899e18e-dd2c-4ec8-865c-094c2fa1317a)

#### Fully connected vs. Fully convolutional
   - Fully connected layer : ì¶œë ¥ì´ ê³ ì •ëœ ë²¡í„°ì´ê³ , ê³µê°„ ì¢Œí‘œë¥¼ ì„ìŒ.
   - Fully convolutional layer : ì¶œë ¥ì´ classification mapì´ê³ , ê³µê°„ ì¢Œí‘œë¥¼ ê°€ì§.
#### Fully Convolutional Networks (FCN) 
   - Fully Convolutional = FCë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì˜¤ì§ Convolutionalë§Œ ì‚¬ìš©í•œë‹¤ëŠ” ëœ».
   - ì„ì˜ì˜ í¬ê¸°ì˜ ì…ë ¥ì´ ë“¤ì–´ì˜¤ë”ë¼ë„ ë§ëŠ” ì¶œë ¥ì„ ë§Œë“¦.
   - skip connectionì„ í†µí•´ ê° ì¸µì˜ ì •ë³´ë¥¼ ë½‘ì•„ì™€ì„œ upsamplingí•˜ì—¬ í•´ìƒë„ë¥¼ ë§ì¶˜ í›„ì—, ì´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ë§Œë“¦.
    ![Screenshot from 2024-08-29 17-18-47](https://github.com/user-attachments/assets/dd5b325c-19c7-4943-a31e-58f84dab89d1)

#### Object detection
- U-Net

   <img src="https://github.com/user-attachments/assets/1d2a9a84-3259-4f65-ad87-8c8400fb967d" width="500"/>

   - contracting path = ì´ë¯¸ì§€ íŠ¹ì§• ì¶•ì†Œ ê³¼ì • (encoder). 3x3 convolutions. ê° levelë§ˆë‹¤ channelì„ 2ë°°ë¡œ ëŠ˜ë¦¼.
   - Expanding path = ì›ë³¸ ì´ë¯¸ì§€ì˜ í•´ìƒë„ë¥¼ ì¶œë ¥ (decoder). 2x2 convolutions. ê° levelë§ˆë‹¤ channelì„ 2ë°°ë¡œ ì¤„ì„.
        - ê° í•´ìƒë„ ë ˆë²¨ì— ë§ëŠ” contracting path featureì„ ê°€ì§€ê³  ì™€ì„œ catì„ í•¨.

- Two-stage detector: R-CNN
   <img src="https://github.com/user-attachments/assets/1f3e2d84-c013-4457-800f-31edacb77357" width="400"/>
   
   - extract region proposal : ë¬¼ì²´ê°€ ì†í•  ìˆ˜ ìˆëŠ” í›„ë³´êµ° (ë…¸ë‘ ë°•ìŠ¤)
   - warped region = extract region proposalì— ë§ê²Œ ì´ë¯¸ì§€ë¥¼ cropí•˜ê³  CNNì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ ì¡°ì ˆ
   - compute CNN feature = ë¶„ë¥˜ë¥¼ ìœ„í•´ ë¯¸ë¦¬ í•™ìŠµëœ CNN ë„¤íŠ¸ì›Œí¬ì— ì´ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ìŒ
   - RNN Family

      <img src="https://github.com/user-attachments/assets/4954221e-9dc4-4859-af89-b4797522f680" width="400"/>

- One-stage detector : YOLO

- One-stage vs. Two-stage
   - ROI poolingì˜ ìœ ë¬´ ì°¨ì´

- RetinaNet
<img src="https://github.com/user-attachments/assets/2f914f3b-e4b8-44c6-b302-43b81eabfd0b" width="500"/>

   - U-netê³¼ ë¹„ìŠ·í•˜ê²Œ feature í”¼ë¼ë¯¸ë“œ í˜•íƒœì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•¨.
   - ê° ìœ„ì¹˜ë§ˆë‹¤ class + box subnetì„ ë‘ì–´ì„œ ë¶„ë¥˜ê³¼ ë°”ìš´ë”© ë°•ìŠ¤ ì˜ˆì¸¡ì„ ì‹œë„í•¨

#### Instance Segmentation
- Mask R-CNN
   <img src="https://github.com/user-attachments/assets/04cadfc9-e4a2-452c-8765-0b250c52f95d" width="500"/>

   - Mask R-CNN = Faster R-CNN + Mask branch
ì‚¬ì§„ì—ì„œ íŒŒë‘ìƒ‰ì´ Mask branchì„. ê·¸ ì™¸ì—ëŠ” Faster R-CNNê³¼ ê°™ìŒ. (ì±„ë„ì´ 80ê°œì´ë¯€ë¡œ 80ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆìŒ.)
   - ROI pooling ëŒ€ì‹ ì— ROIAlignì„ ì‚¬ìš©.

#### Transformer-based methods
- DETR
   <img src="https://github.com/user-attachments/assets/bf1ae1f0-7786-4e4e-a9fa-d5d28d256344" width="500"/>
   - non-maximum suppression ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ë˜ê²Œ í•¨ (ëª¨ë¸ ë‚´ì— ë“¤ì–´ê°.)
   - íŠ¸ë ŒìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ì‚¬ìš©í•¨.

- MaskFormer
   <img src="https://github.com/user-attachments/assets/364b0dca-8941-476d-bc8c-c2dde6b3ea1c" width="500"/>
   - ì„¸ê·¸ë©˜í…Œì´ì…˜ì—ì„œë„ Transformerê°€ ì‚¬ìš©ë¨.
   - semantic- and instance- segmentationsì„ ê°œë³„ì ìœ¼ë¡œ ë³´ëŠ” ê²Œ ì•„ë‹ˆë¼ Mask classificationìœ¼ë¡œ í•˜ë‚˜ë¡œ í†µí•©.

#### Segmentation foundation model
- SAM : Segment Anything Model
   <img src="https://github.com/user-attachments/assets/42755154-f8be-457c-9a03-80e65165156b" width="500"/>
   - íŠ¹ë³„í•œ ì¶”ê°€ í•™ìŠµ ì—†ì´ë„ ì–´ë–¤ ê°ì²´ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜í•  ìˆ˜ ìˆìŒ.

### [CV ì´ë¡ ] 05. Computational Imaging
#### Computational Imaging
- Image restoration - denoising
   - ì´ë¯¸ì§€ì˜ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë³µì›.
   - $y = x + n$, ë…¸ì´ì¦ˆ ìˆëŠ” ì´ë¯¸ì§€ $y$ëŠ” ê¹¨ë—í•œ ì´ë¯¸ì§€ $x$ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ $n$ì´ ë”í•´ì§„ë‹¤ê³  ê°€ì • ($ğ‘›$~$ğ‘(0, ğœ 2 )$)

      <img src="https://github.com/user-attachments/assets/3cd05968-b610-465b-814c-60eadfc0a086" width="500"/>

- Image super resolution
   - ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³µì›.
   - ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ì•„ì„œ ê° ì´ë¯¸ì§€ì— í•´ë‹¹í•˜ëŠ” ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ë§Œë“¦.
    - ì´ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ Down-sampling ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©
      - ë” ì •í™•í•œ ë°ì´í„°ë¥¼ ì·¨ë“í•˜ê¸° ìœ„í•´ì„œ RealSR ë…¼ë¬¸ì—ì„œëŠ” ì‹¤ì œ ì¹´ë©”ë¼ì™€ ì´ë¯¸ì§€ì— ë§ëŠ” ë‹¤ìš´ìƒ˜í”Œë§ ê¸°ë²•ì„ ì†Œê°œí•¨.

- Image deblurring
   - deblurring ì—­ì‹œ í•©ì„± ë°ì´í„°ë¥¼ ì´ìš©í•¨.
   -  Blur ì»¤ë„ì´ë¼ëŠ” í•„í„°ë¥¼ ì„ í˜• ë“±ìœ¼ë¡œ ë¬˜ì‚¬ë¥¼ í•´ì„œ íŠ¹ì •í•œ ë°©í–¥ìœ¼ë¡œ ë¸”ëŸ¬ë¥¼ ë§Œë“¦

#### Advanced loss functions
- L2 (MSE) or L1 loss functionsì€ ì§€ê°ì ìœ¼ë¡œ ì˜ ì •ë ¬ë˜ì§€ ì•Šì€ (not perceptually well-aligned) lossì„.

   <img src="https://github.com/user-attachments/assets/d1b94f46-d9a9-4c40-b502-e7d9b48408df" width="300"/>

   - ê°™ì€ lossì„ì—ë„ GTì— ë¹„ìŠ·í•œ ì´ë¯¸ì§€ì™€ ê·¸ë ‡ì§€ ì•Šì€ ì´ë¯¸ì§€ê°€ ìˆìŒ.

- Adversarial loss (GAN)
   - Colorizingì„ í•  ë•Œ, ì´ë¯¸ì§€ëŠ” í‘ê³¼ ë°± ë‘ ê°œ ë°–ì— ì—†ìŒ.ì´ë¥¼ L2ë¡œ í•˜ë©´, íšŒìƒ‰ì´ë¯¸ì§€ë¥¼ ë±‰ì–´ëƒ„. (ê°€ì¥ Lossê°€ ì‘ìœ¼ë¯€ë¡œ?)
    
   - í•˜ì§€ë§Œ, Adversarial lossë¥¼ ì ìš©í•˜ë©´, ì´ íšŒìƒ‰ ì´ë¯¸ì§€ê°€ fake dataë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ. (í•œë§ˆë””ë¡œ, real dataì™€ ë¹„ìŠ·í•œ í˜•íƒœ ì¶œë ¥ì„ ë§Œë“¦.)

      <img src="https://github.com/user-attachments/assets/9a832821-d990-4af7-845f-fe1237f1f93f" width="300"/>
   - ë³´í†µ Adversarial loss ì‚¬ìš© ì‹œ, Pixel-wise MSE loss ë“±ê³¼ í•¨ê»˜ ì‚¬ìš©
- Perceptual loss
   - ì‚¬ì „ í•™ìŠµëœ filterê°€ ì‚¬ëŒ ì‹œê°ê³¼ ìœ ì‚¬í•˜ë‹¤ëŠ” ê°€ì •
   <img src="https://github.com/user-attachments/assets/49f79a5e-1a34-4e6f-8fd9-24a6a6c69375" width="300"/>
      - lmage transform net : ì…ë ¥ì—ì„œ ë³€í™˜ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•¨.
      - Loss network : ìƒì„±ëœ ì´ë¯¸ì§€ì™€ target ì‚¬ì´ì˜ loss ê³„ì‚°. (ì¼ë°˜ì ìœ¼ë¡œ VGG modelì´ ì‚¬ìš©ë¨.)
         - lmage transform net í›ˆë ¨ ì‹œ, fixë¨.

- Adversarial loss vs. Perceptual loss
   -  Adversarial loss : í•™ìŠµ ì½”ë“œê°€ ë³µì¡í•¨. í•˜ì§€ë§Œ, ì‚¬ì „ êµìœ¡ëœ ë„¤íŠ¸ì›Œí¬ í•„ìš” ì—†ìœ¼ë©° ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì ìš©í•  ìˆ˜ ìˆìŒ
   - Perceptual loss :í•™ìŠµ ì½”ë“œê°€ ì‰¬ì›€. ì‚¬ì „ í›ˆë ¨ëœ ë„¤íŠ¸ì›Œí¬ê°€ í•„ìš”
### [CV ì´ë¡ ] ê³¼ì œ 1 : Understanding Vision Transformers
- timm (PyTorch Image Models)
   - PyTorch ê¸°ë°˜ì˜ ì´ë¯¸ì§€ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
   - ë‹¤ì–‘í•œ ì‚¬ì „ í•™ìŠµëœ ë¹„ì „ ëª¨ë¸ë“¤ì„ ì œê³µ (torchivisionì—ì„œ ì œê³µí•˜ëŠ” pretrained modelë³´ë‹¤ ë” ë§ì€ ëª¨ë¸ì„ ì œê³µí•œë‹¤ê³  í•¨!)
   - ì„¤ì¹˜ : pip install timm
- Position embedding ì‹œê°í™” (cosine similarity)

   <img src="https://github.com/user-attachments/assets/73e2e335-274c-470e-9d0b-7fd00ad725c1" width="300"/>
   - ê° íŒ¨ì¹˜ë§ˆë‹¤ì˜ Position embeddingì„ ì‹œê°í™”í•œ ê²ƒ. ìƒ‰ì´ ë…¸ë‘ìƒ‰ì— ê°€ê¹Œìš¸ìˆ˜ë¡ attentionì´ ë†’ìŒ. 
   - ê° íŒ¨ì¹˜ ìœ„ì¹˜ì— ëŒ€í•œ attentionì´ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ.
- Attention Matrix ì‹œê°í™” (3ë²ˆì§¸ ë©€í‹° í•´ë“œ ì˜ˆì‹œ)
   - `attention_matrix = torch.matmul(q, kT)`

      <img src="https://github.com/user-attachments/assets/ea6cd5c9-829d-491d-bdf3-afe12197ec90" width="300"/>
   - 100 ~ 125ì—ì„œ attentionì´ ê°•í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ.
   - **softmax(q, kT)ë¥¼ í•˜ì§€ ì•ŠëŠ” ì´ìœ **
      - softmaxëŠ” Attention Scoreë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ visualizationì´ ì‰½ì§€ ì•ŠìŒ.
      - ë”°ë¼ì„œ, softmax Temperatureì„ ì„¤ì •í•˜ì—¬ softmaxë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŒ.
      - softmax Temperature

         <img src="https://github.com/user-attachments/assets/5f4d3889-6694-452d-8a18-a0f094e7a8ce" alt="1_p1iKxUJcXDlSEZCpMCwNgg" width="300"/>

         - Temperatureê°€ 1ì¼ ë•Œ,

            <img src="https://github.com/user-attachments/assets/b5cef290-c869-4f1e-bdde-97f238381801" width="300"/>
         - Temperatureê°€ 10ì¼ ë•Œ,

            <img src="https://github.com/user-attachments/assets/975abb1b-d67a-43f7-8dbb-eabd504aa2f7" width="300"/>
         - Temperatureê°€ 30ì¼ ë•Œ,

            <img src="https://github.com/user-attachments/assets/975abb1b-d67a-43f7-8dbb-eabd504aa2f7" width="300"/>

### [CV ì´ë¡ ] ê³¼ì œ 1 : Understanding Vision Transformers           
   - pytorch-lightning
      - PyTorchì— ëŒ€í•œ High-level ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ Python ë¼ì´ë¸ŒëŸ¬ë¦¬
      - ì„¤ì¹˜ : pip install pytorch-lightning

- logits
   ```python
   def forward(self, pixel_values):
        outputs = self.vit(pixel_values=pixel_values)
        return outputs.logits
   ```
   - logitsì€ ì†Œí”„íŠ¸ë§¥ìŠ¤(Softmax) ë˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ(Sigmoid) í•¨ìˆ˜ê°€ ì ìš©ë˜ê¸° ì „ì˜ ì›ì‹œ ì ìˆ˜ì„ ì˜ë¯¸í•¨.
   
- nn.Module í´ë˜ìŠ¤ / pl.LightningModule í´ë˜ìŠ¤ì—ì„œì˜ self
   ```python
   def common_step(self, batch, batch_idx):
        pixel_values = batch['pixel_values']
        labels = batch['labels']
        logits = self(pixel_values)
        criterion = nn.CrossEntropyLoss()
        loss = criterion(logits, labels)
   ```
   - ì—¬ê¸°ì„œ logits = self(pixel_values)ëŠ” forwardì„ í˜¸ì¶œí•˜ì—¬ pixel_valuesë¥¼ ì²˜ë¦¬
      - selfëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì˜ë¯¸í•˜ê³  ì´ëŠ” pl.LightningModule í˜¹ì€ nn.modelì— ì˜í•´ ìë™ìœ¼ë¡œ forward ë©”ì„œë“œê°€ ì‹¤í–‰í•˜ë¯€ë¡œ `self(pixel_values)`ëŠ” forward í˜¸ì¶œ

- `nn.CrossEntropyLoss()`
   - ìœ„ ì½”ë“œì—ì„œ softmax ê°’ì´ ì•„ë‹Œ logitsë¡œ lossë¥¼ ê³„ì‚°í•˜ëŠ” ì´ìœ 
      - `nn.CrossEntropyLoss()`ì— softmaxê°€ ë“¤ì–´ê°€ ìˆìœ¼ë¯€ë¡œ softmaxì˜ í™•ë¥ ê°’ì´ ì•„ë‹Œ, logitsìœ¼ë¡œ ê³„ì‚°